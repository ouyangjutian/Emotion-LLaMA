#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AffectGPT å¤šæ¨¡æ€ç‰¹å¾é¢„æå–è„šæœ¬
ç±»ä¼¼äºEmotion-LLaMAçš„é¢„æå–æ–¹å¼ï¼Œå‡å°‘è®­ç»ƒæ—¶çš„æ˜¾å­˜æ¶ˆè€—

æ”¯æŒçš„ç‰¹å¾ç±»å‹:
- Frame: CLIP-ViT-Largeç¼–ç çš„è§†é¢‘å¸§ç‰¹å¾
- Face: CLIP-ViT-Largeç¼–ç çš„äººè„¸ç‰¹å¾  
- Audio: HuBERT-Largeç¼–ç çš„éŸ³é¢‘ç‰¹å¾
"""

import os
import sys
import argparse
import numpy as np
import torch
from tqdm import tqdm
from torch.utils.data import DataLoader
import warnings
warnings.filterwarnings("ignore")

# æ·»åŠ è·¯å¾„
sys.path.append('.')
sys.path.append('./my_affectgpt')

# å¯¼å…¥CLIPç”¨äºfine_grained_descriptionsç¼–ç 
try:
    import clip
    CLIP_AVAILABLE = True
except ImportError:
    CLIP_AVAILABLE = False
    print("âš ï¸ Warning: CLIP not installed. AU descriptions encoding will be skipped.")
    print("   Install with: pip install git+https://github.com/openai/CLIP.git")

from my_affectgpt.common.registry import registry
from my_affectgpt.models.encoder import *
from my_affectgpt.processors.video_processor import load_video, load_face
from my_affectgpt.models.ImageBind.data import transform_audio, load_audio
import config


class FeatureExtractor:
    """å¤šæ¨¡æ€ç‰¹å¾æå–å™¨"""
    
    def __init__(self, device='cuda:0', mer_factory_output_root=None):
        self.device = device
        self.encoders = {}
        self.multi_fusion_model = None
        self.mer_factory_output_root = mer_factory_output_root  # MER-Factoryè¾“å‡ºæ ¹ç›®å½•
        self.clip_model = None  # CLIPæ¨¡å‹ç”¨äºAU descriptionsç¼–ç 
        
    def load_visual_encoder(self, encoder_name='CLIP_VIT_LARGE', quiet=False):
        """åŠ è½½è§†è§‰ç¼–ç å™¨ (Frame/Face)"""
        if not quiet:
            print(f'ğŸ”§ Loading Visual Encoder: {encoder_name}')
        encoder_cls = registry.get_visual_encoder_class(encoder_name)
        encoder = encoder_cls().to(self.device)
        encoder.eval()
        self.encoders['visual'] = encoder
        return encoder
        
    def load_acoustic_encoder(self, encoder_name='HUBERT_LARGE', quiet=False):
        """åŠ è½½å£°å­¦ç¼–ç å™¨ (Audio)"""
        if not quiet:
            print(f'ğŸ”§ Loading Acoustic Encoder: {encoder_name}')
        encoder_cls = registry.get_acoustic_encoder_class(encoder_name)
        encoder = encoder_cls().to(self.device)
        encoder.eval()
        self.encoders['acoustic'] = encoder
        return encoder
    
    def load_multi_fusion_model(self, model_config_path=None, quiet=False):
        """åŠ è½½Multièåˆæ¨¡å‹ (ç”¨äºFace+Audioâ†’Multi) - ä½¿ç”¨é¢„è®­ç»ƒæƒé‡"""
        if not quiet:
            print(f'ğŸ”§ Loading Multi Fusion Model with pretrained weights')
        
        try:
            # å¯¼å…¥å¿…è¦çš„æ¨¡å—
            from my_affectgpt.models.affectgpt import AffectGPT
            from omegaconf import OmegaConf
            import copy
            
            # åŠ è½½é…ç½®æ–‡ä»¶
            if model_config_path is None:
                model_config_path = './train_configs/emercoarse_highlevelfilter4_outputhybird_bestsetup_bestfusion_lz.yaml'
            
            cfg = OmegaConf.load(model_config_path)
            model_cfg = copy.deepcopy(cfg.model)
            
            # ğŸ¯ å…³é”®ä¿®å¤ï¼šä¸ºäº†åŠ è½½Multièåˆç»„ä»¶ï¼Œä¸´æ—¶ç¦ç”¨skip_encoders
            # è¿™æ ·å¯ä»¥ç¡®ä¿æ‰€æœ‰Multièåˆç›¸å…³çš„ç»„ä»¶éƒ½èƒ½æ­£ç¡®åˆå§‹åŒ–
            original_skip_encoders = model_cfg.get('skip_encoders', False)
            model_cfg.skip_encoders = False  # ä¸´æ—¶ç¦ç”¨ï¼Œç¡®ä¿Multiç»„ä»¶æ­£ç¡®åŠ è½½
            
            if not quiet:
                print(f'ğŸ”§ Temporarily enabling encoders for Multi fusion model loading')
            
            # åˆ›å»ºAffectGPTæ¨¡å‹å®ä¾‹
            temp_model = AffectGPT.from_config(model_cfg)
            
            # å°è¯•åŠ è½½é¢„è®­ç»ƒæƒé‡ (å¦‚æœæœ‰çš„è¯)
            # è¿™é‡Œå¯ä»¥åŠ è½½checkpointï¼Œä½†ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬ä½¿ç”¨åˆå§‹åŒ–çš„æƒé‡
            temp_model = temp_model.to(self.device)
            temp_model.eval()
            
            # éªŒè¯Multièåˆç»„ä»¶æ˜¯å¦æ­£ç¡®åŠ è½½
            if not hasattr(temp_model, 'multi_video_embs') or not hasattr(temp_model, 'multi_audio_embs'):
                raise RuntimeError("Multi fusion components not properly initialized")
            
            # æå–Multièåˆç›¸å…³çš„ç»„ä»¶
            self.multi_fusion_model = {
                'multi_fusion_type': temp_model.multi_fusion_type,
                'max_hidden_size': temp_model.max_hidden_size,
                'multi_video_embs': temp_model.multi_video_embs,
                'multi_audio_embs': temp_model.multi_audio_embs,
            }
            
            # æ ¹æ®èåˆç±»å‹æ·»åŠ ç›¸åº”ç»„ä»¶
            if temp_model.multi_fusion_type == 'attention':
                if not hasattr(temp_model, 'attention_mlp') or not hasattr(temp_model, 'fc_att'):
                    raise RuntimeError("Multi attention components not properly initialized")
                    
                self.multi_fusion_model.update({
                    'attention_mlp': temp_model.attention_mlp,
                    'fc_att': temp_model.fc_att,
                })
                
            elif temp_model.multi_fusion_type == 'qformer':
                if not hasattr(temp_model, 'multi_query_tokens') or not hasattr(temp_model, 'multi_Qformer'):
                    raise RuntimeError("Multi Q-Former components not properly initialized")
                    
                self.multi_fusion_model.update({
                    'multi_query_tokens': temp_model.multi_query_tokens,
                    'multi_Qformer': temp_model.multi_Qformer,
                    'multi_position_embedding': temp_model.multi_position_embedding
                })
            
            # æ¸…ç†ä¸´æ—¶æ¨¡å‹ä»¥é‡Šæ”¾æ˜¾å­˜
            del temp_model
            import torch
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            if not quiet:
                print(f'âœ… Multi fusion model loaded successfully: {self.multi_fusion_model["multi_fusion_type"]} type')
                print(f'   max_hidden_size: {self.multi_fusion_model["max_hidden_size"]}')
                print(f'ğŸ¯ Using COMPLETE version - identical to real-time mode!')
            
            return True
            
        except Exception as e:
            if not quiet:
                print(f'âš ï¸ Failed to load multi fusion model: {e}')
                print('   Will use simplified fallback method')
                import traceback
                traceback.print_exc()
            return False
    
    def extract_frame_features(self, video_path, n_frms=8, sampling='uniform', video_name=None):
        """æå–Frameç‰¹å¾
        
        Args:
            video_path: è§†é¢‘è·¯å¾„
            n_frms: é‡‡æ ·å¸§æ•°
            sampling: é‡‡æ ·ç­–ç•¥ (uniform/headtail/emotion_peak)
            video_name: è§†é¢‘åç§°ï¼ˆemotion_peakæ¨¡å¼éœ€è¦ï¼Œç”¨äºåŠ è½½au_infoï¼‰
        
        Returns:
            frame_features: [T, D] ç‰¹å¾çŸ©é˜µ
        """
        try:
            # ğŸ¯ å¦‚æœæ˜¯emotion_peakä¸”æä¾›äº†video_nameï¼Œä½¿ç”¨æ™ºèƒ½é‡‡æ ·
            if sampling == 'emotion_peak' and video_name:
                return self.extract_frame_features_smart(video_path, video_name, n_frms=8)
            
            # æ ‡å‡†é‡‡æ ·ï¼šuniform æˆ– headtail
            raw_frame, _ = load_video(
                video_path=video_path,
                n_frms=n_frms,
                height=224,
                width=224,
                sampling=sampling,
                return_msg=True
            )
            
            # ğŸ¯ ä¸å®æ—¶æ¨¡å¼å®Œå…¨ä¸€è‡´çš„æ•°æ®å¤„ç†
            # å®æ—¶æ¨¡å¼ä½¿ç”¨: alpro_video_train (åŒ…å«RandomResizedCropVideo)
            # é¢„æå–æ¨¡å¼: ä½¿ç”¨ç›¸åŒçš„alpro_video_train + å›ºå®šéšæœºç§å­ç¡®ä¿å¯å¤ç°æ€§
            
            from my_affectgpt.processors.video_processor import AlproVideoTrainProcessor
            import torch
            import random
            import numpy as np
            
            # ğŸ”‘ å…³é”®ï¼šä¸ºæ¯ä¸ªæ ·æœ¬è®¾ç½®å›ºå®šä½†å”¯ä¸€çš„éšæœºç§å­
            # è¿™æ ·æ—¢ä¿è¯äº†ä¸å®æ—¶æ¨¡å¼ç›¸åŒçš„å¤„ç†é€»è¾‘ï¼Œåˆç¡®ä¿äº†é¢„æå–ç‰¹å¾çš„å¯å¤ç°æ€§
            sample_identifier = f"{video_path}_{n_frms}_{sampling}"
            sample_seed = hash(sample_identifier) % (2**32)
            
            # è®¾ç½®å›ºå®šéšæœºç§å­
            torch.manual_seed(sample_seed)
            random.seed(sample_seed)
            np.random.seed(sample_seed)
            
            # ä½¿ç”¨ä¸å®æ—¶æ¨¡å¼å®Œå…¨ç›¸åŒçš„trainå¤„ç†å™¨
            # ğŸ¯ é‡è¦ï¼šå‚æ•°å¿…é¡»ä¸è®­ç»ƒé…ç½®æ–‡ä»¶å®Œå…¨ä¸€è‡´
            train_processor = AlproVideoTrainProcessor(
                image_size=224,     # ä¸é…ç½®æ–‡ä»¶ vis_processor.train.image_size ä¸€è‡´
                n_frms=n_frms,      # åŠ¨æ€è®¾ç½®
                min_scale=0.5,      # AlproVideoTrainProcessoré»˜è®¤å€¼
                max_scale=1.0,      # AlproVideoTrainProcessoré»˜è®¤å€¼
                mean=None,          # ä½¿ç”¨é»˜è®¤ImageNetå‚æ•°
                std=None            # ä½¿ç”¨é»˜è®¤ImageNetå‚æ•°
            )
            frame = train_processor.transform(raw_frame)  # ä¸å®æ—¶æ¨¡å¼å®Œå…¨ä¸€è‡´ï¼
            frame = frame.unsqueeze(0).to(self.device)  # [1, C, T, H, W]
            raw_frame = raw_frame.unsqueeze(0).to(self.device)  # [1, C, T, H, W]
            
            # ç‰¹å¾æå–
            with torch.no_grad():
                features = self.encoders['visual'](frame, raw_frame)  # [1, T, D]
                features = features.squeeze(0).cpu().numpy()  # [T, D]
            
            return features
            
        except Exception as e:
            print(f"Error extracting frame features from {video_path}: {e}")
            return None
    
    def extract_frame_features_smart(self, video_path, video_name, n_frms=8):
        """åŸºäºau_infoæ™ºèƒ½é‡‡æ ·æå–Frameç‰¹å¾ï¼ˆå›ºå®š8å¸§ï¼‰
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            video_name: è§†é¢‘åç§°ï¼ˆç”¨äºæŸ¥æ‰¾au_infoï¼‰
            n_frms: å›ºå®šä¸º8å¸§
        
        Returns:
            features: [8, D] çš„ç‰¹å¾çŸ©é˜µ
        """
        try:
            # 1. åŠ è½½au_info
            au_info = self.load_au_info(video_name)
            
            # 2. å…ˆåŠ è½½æ•´ä¸ªè§†é¢‘ä»¥è·å–æ€»å¸§æ•°
            import cv2
            cap = cv2.VideoCapture(video_path)
            total_video_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            cap.release()
            
            if total_video_frames == 0:
                print(f"âš ï¸ Warning: Cannot get frame count from {video_path}")
                # å›é€€åˆ°å‡åŒ€é‡‡æ ·
                return self.extract_frame_features(video_path, n_frms=8, sampling='uniform')
            
            # 3. è®¡ç®—æ™ºèƒ½é‡‡æ ·çš„å¸§ç´¢å¼•
            frame_indices = self.calculate_smart_frame_indices(au_info, total_video_frames)
            
            # 4. ä½¿ç”¨è‡ªå®šä¹‰ç´¢å¼•åŠ è½½è§†é¢‘å¸§
            from my_affectgpt.processors.video_processor import load_video_with_indices
            import torch
            import random
            import numpy as np
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ load_video_with_indices å‡½æ•°ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨æ›¿ä»£æ–¹æ¡ˆ
            try:
                raw_frame = load_video_with_indices(
                    video_path=video_path,
                    frame_indices=frame_indices,
                    height=224,
                    width=224
                )
            except (ImportError, AttributeError):
                # å¦‚æœæ²¡æœ‰ load_video_with_indicesï¼Œæ‰‹åŠ¨åŠ è½½æŒ‡å®šå¸§
                raw_frame = self._load_specific_frames(video_path, frame_indices, height=224, width=224)
            
            # 5. æ•°æ®å¤„ç†ï¼ˆä¸å®æ—¶æ¨¡å¼ä¸€è‡´ï¼‰
            from my_affectgpt.processors.video_processor import AlproVideoTrainProcessor
            
            sample_identifier = f"{video_path}_{video_name}_smart8"
            sample_seed = hash(sample_identifier) % (2**32)
            
            torch.manual_seed(sample_seed)
            random.seed(sample_seed)
            np.random.seed(sample_seed)
            
            train_processor = AlproVideoTrainProcessor(
                image_size=224,
                n_frms=8,
                min_scale=0.5,
                max_scale=1.0,
                mean=None,
                std=None
            )
            
            frame = train_processor.transform(raw_frame)
            frame = frame.unsqueeze(0).to(self.device)
            raw_frame = raw_frame.unsqueeze(0).to(self.device)
            
            # 6. ç‰¹å¾æå–
            with torch.no_grad():
                features = self.encoders['visual'](frame, raw_frame)
                features = features.squeeze(0).cpu().numpy()
            
            return features
            
        except Exception as e:
            print(f"âš ï¸ Error in smart sampling for {video_path}: {e}")
            print(f"   Falling back to uniform sampling")
            import traceback
            traceback.print_exc()
            # å›é€€åˆ°å‡åŒ€é‡‡æ ·
            return self.extract_frame_features(video_path, n_frms=8, sampling='uniform')
    
    def _load_specific_frames(self, video_path, frame_indices, height=224, width=224):
        """æ‰‹åŠ¨åŠ è½½è§†é¢‘çš„æŒ‡å®šå¸§
        
        Args:
            video_path: è§†é¢‘è·¯å¾„
            frame_indices: è¦åŠ è½½çš„å¸§ç´¢å¼•åˆ—è¡¨ (0-indexed)
            height, width: ç›®æ ‡å°ºå¯¸
        
        Returns:
            torch.Tensor: [C, T, H, W] æ ¼å¼çš„è§†é¢‘å¸§
        """
        import cv2
        import torch
        import numpy as np
        from torchvision import transforms
        
        cap = cv2.VideoCapture(video_path)
        frames = []
        
        for frame_idx in sorted(frame_indices):
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
            ret, frame = cap.read()
            
            if ret:
                # BGR to RGB
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                # Resize
                frame = cv2.resize(frame, (width, height))
                frames.append(frame)
            else:
                # å¦‚æœè¯»å–å¤±è´¥ï¼Œä½¿ç”¨é»‘è‰²å¸§
                frames.append(np.zeros((height, width, 3), dtype=np.uint8))
        
        cap.release()
        
        # è½¬æ¢ä¸ºtorch tensor [T, H, W, C]
        frames = np.stack(frames, axis=0)
        # è½¬æ¢ä¸º [C, T, H, W]
        frames = torch.from_numpy(frames).permute(3, 0, 1, 2).float()
        # å½’ä¸€åŒ–åˆ° [0, 1]
        frames = frames / 255.0
        
        return frames
    
    def load_au_info(self, video_name):
        """ä»MER-Factoryçš„JSONæ–‡ä»¶åŠ è½½au_info
        
        Args:
            video_name: è§†é¢‘åç§°ï¼ˆä¸å«æ‰©å±•åï¼‰
        
        Returns:
            au_infoå­—å…¸ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æˆ–æ— au_infoåˆ™è¿”å›None
        """
        if not self.mer_factory_output_root:
            return None
        
        import json
        from pathlib import Path
        
        # æ„å»ºJSONæ–‡ä»¶è·¯å¾„
        json_path = Path(self.mer_factory_output_root) / video_name / f"{video_name}_au_analysis.json"
        
        if not json_path.exists():
            return None
        
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            return data.get('au_info')
        except Exception as e:
            print(f"âš ï¸ Warning: Failed to load au_info from {json_path}: {e}")
            return None
    
    def calculate_smart_frame_indices(self, au_info, total_video_frames):
        """æ ¹æ®au_infoæ™ºèƒ½è®¡ç®—éœ€è¦é‡‡æ ·çš„8å¸§ç´¢å¼•
        
        Args:
            au_info: au_infoå­—å…¸
            total_video_frames: è§†é¢‘æ€»å¸§æ•°
        
        Returns:
            sorted list of 8 frame indices (0-indexed)
        """
        if not au_info or 'peak_frames' not in au_info or len(au_info['peak_frames']) == 0:
            # æ— au_infoï¼Œå›é€€åˆ°å‡åŒ€é‡‡æ ·
            import numpy as np
            indices = np.linspace(0, total_video_frames - 1, 8).astype(int).tolist()
            return sorted(indices)
        
        # è·å–ç¬¬ä¸€ä¸ªå³°å€¼å¸§ä¿¡æ¯ï¼ˆå¦‚æœæœ‰å¤šä¸ªå³°å€¼ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªï¼‰
        peak_info = au_info['peak_frames'][0]
        peak_index = peak_info['peak_index']  # 0-indexed
        frames_before = peak_info['frames_before_peak']
        frames_after = peak_info['frames_after_peak']
        total_frames = au_info['total_frames']
        
        selected_indices = set()
        
        # 1. å³°å€¼å¸§å¿…å®šé‡‡å–
        selected_indices.add(peak_index)
        
        # 2. æ ¹æ®ç­–ç•¥é€‰æ‹©é‚»è¿‘å¸§
        if frames_before >= 2 and frames_after >= 2:
            # ç­–ç•¥1ï¼šå‰åéƒ½è‡³å°‘æœ‰2å¸§
            # é‡‡å–å³°å€¼å¸§å‰é¢æŒ¨ç€çš„2å¸§
            if peak_index >= 1:
                selected_indices.add(peak_index - 1)
            if peak_index >= 2:
                selected_indices.add(peak_index - 2)
            # é‡‡å–å³°å€¼å¸§åé¢æŒ¨ç€çš„2å¸§
            if peak_index + 1 < total_frames:
                selected_indices.add(peak_index + 1)
            if peak_index + 2 < total_frames:
                selected_indices.add(peak_index + 2)
            # å·²é‡‡å–5å¸§ï¼Œè¿˜éœ€è¦3å¸§
            remaining_needed = 8 - len(selected_indices)
        
        elif (frames_before == 1 and frames_after >= 2) or (frames_before >= 2 and frames_after == 1):
            # ç­–ç•¥2ï¼šä¸€è¾¹ä¸º1å¸§ï¼Œå¦ä¸€è¾¹è‡³å°‘2å¸§
            if frames_before == 1:
                # å·¦è¾¹åªæœ‰1å¸§ï¼Œé‡‡å–å®ƒ
                if peak_index >= 1:
                    selected_indices.add(peak_index - 1)
                # å³è¾¹é‡‡å–æŒ¨ç€çš„2å¸§
                if peak_index + 1 < total_frames:
                    selected_indices.add(peak_index + 1)
                if peak_index + 2 < total_frames:
                    selected_indices.add(peak_index + 2)
            else:  # frames_after == 1
                # å³è¾¹åªæœ‰1å¸§ï¼Œé‡‡å–å®ƒ
                if peak_index + 1 < total_frames:
                    selected_indices.add(peak_index + 1)
                # å·¦è¾¹é‡‡å–æŒ¨ç€çš„2å¸§
                if peak_index >= 1:
                    selected_indices.add(peak_index - 1)
                if peak_index >= 2:
                    selected_indices.add(peak_index - 2)
            # å·²é‡‡å–4å¸§ï¼Œè¿˜éœ€è¦4å¸§
            remaining_needed = 8 - len(selected_indices)
        
        elif frames_before == 1 and frames_after == 1:
            # ç­–ç•¥3ï¼šå‰åéƒ½åªæœ‰1å¸§
            if peak_index >= 1:
                selected_indices.add(peak_index - 1)
            if peak_index + 1 < total_frames:
                selected_indices.add(peak_index + 1)
            # å·²é‡‡å–3å¸§ï¼Œè¿˜éœ€è¦5å¸§
            remaining_needed = 8 - len(selected_indices)
        
        elif frames_before == 0 or frames_after == 0:
            # ç­–ç•¥4ï¼šä¸€è¾¹ä¸º0å¸§
            if frames_before == 0:
                # å·¦è¾¹æ²¡æœ‰å¸§ï¼Œå³è¾¹é‡‡å–æŒ¨ç€çš„2å¸§
                if peak_index + 1 < total_frames:
                    selected_indices.add(peak_index + 1)
                if peak_index + 2 < total_frames:
                    selected_indices.add(peak_index + 2)
            else:  # frames_after == 0
                # å³è¾¹æ²¡æœ‰å¸§ï¼Œå·¦è¾¹é‡‡å–æŒ¨ç€çš„2å¸§
                if peak_index >= 1:
                    selected_indices.add(peak_index - 1)
                if peak_index >= 2:
                    selected_indices.add(peak_index - 2)
            # å·²é‡‡å–3å¸§ï¼Œè¿˜éœ€è¦5å¸§
            remaining_needed = 8 - len(selected_indices)
        
        else:
            # é»˜è®¤ï¼šå‡åŒ€é‡‡æ ·å‰©ä½™å¸§
            remaining_needed = 8 - len(selected_indices)
        
        # 3. ä»æœªé€‰æ‹©çš„å¸§ä¸­å‡åŒ€é‡‡æ ·å‰©ä½™éœ€è¦çš„å¸§
        if remaining_needed > 0:
            available_indices = [i for i in range(total_frames) if i not in selected_indices]
            
            if len(available_indices) > 0:
                import numpy as np
                # å‡åŒ€é‡‡æ ·
                if len(available_indices) <= remaining_needed:
                    # å¯ç”¨å¸§ä¸å¤Ÿï¼Œå…¨éƒ¨é‡‡ç”¨
                    selected_indices.update(available_indices)
                else:
                    # å‡åŒ€é‡‡æ ·
                    step = len(available_indices) / remaining_needed
                    for i in range(remaining_needed):
                        idx = int(i * step)
                        if idx < len(available_indices):
                            selected_indices.add(available_indices[idx])
        
        # 4. ç¡®ä¿æœ‰8å¸§ï¼ˆå¦‚æœè§†é¢‘å¤ªçŸ­ï¼‰
        while len(selected_indices) < 8 and len(selected_indices) < total_frames:
            # æ·»åŠ ç¼ºå¤±çš„å¸§ï¼ˆä»æœªé€‰æ‹©çš„å¸§ä¸­é¡ºåºé€‰æ‹©ï¼‰
            available = [i for i in range(total_frames) if i not in selected_indices]
            if available:
                selected_indices.add(available[0])
            else:
                break
        
        # 5. å¦‚æœè¿˜ä¸å¤Ÿ8å¸§ï¼ˆè§†é¢‘æ€»å¸§æ•°<8ï¼‰ï¼Œå¾ªç¯é‡å¤å·²æœ‰å¸§
        result_indices = sorted(list(selected_indices))
        if len(result_indices) < 8:
            # ä½¿ç”¨å¾ªç¯é‡å¤ç­–ç•¥ï¼Œæ›´å‡åŒ€åœ°åˆ†å¸ƒå¸§
            original_indices = result_indices.copy()
            while len(result_indices) < 8:
                # å¾ªç¯é‡å¤æ‰€æœ‰å·²é€‰å¸§ï¼Œè€Œä¸æ˜¯åªé‡å¤æœ€åä¸€å¸§
                for idx in original_indices:
                    if len(result_indices) >= 8:
                        break
                    result_indices.append(idx)
            result_indices.sort()  # é‡æ–°æ’åºä¿æŒæ—¶åº
        
        return result_indices[:8]  # ç¡®ä¿åªè¿”å›8å¸§
    
    def extract_face_features(self, face_npy_path, n_frms=8):
        """ğŸ¯ ä¿®å¤ï¼šç¡®ä¿ä¸å®æ—¶æ¨¡å¼å®Œå…¨ä¸€è‡´çš„Faceç‰¹å¾æå–"""
        try:
            # åŠ è½½äººè„¸æ•°æ®ï¼ˆä¸å®æ—¶æ¨¡å¼ç›¸åŒï¼‰
            raw_face, _ = load_face(
                face_npy=face_npy_path,
                n_frms=n_frms,
                height=224,
                width=224,
                sampling="uniform",
                return_msg=True
            )
            
            # å¯¼å…¥å¿…è¦çš„æ¨¡å—
            import torch
            import random
            import numpy as np
            from my_affectgpt.processors.video_processor import AlproVideoTrainProcessor
            
            # ğŸ¯ å…³é”®ä¿®å¤ï¼šä½¿ç”¨ä¸å®æ—¶æ¨¡å¼å®Œå…¨ç›¸åŒçš„é¢„å¤„ç†å™¨
            # å®æ—¶æ¨¡å¼ä½¿ç”¨ vis_processor.transform()ï¼Œé¢„æå–æ¨¡å¼ä¹Ÿå¿…é¡»ä½¿ç”¨ç›¸åŒå¤„ç†
            if not hasattr(self, 'vis_processor'):
                # åŠ è½½ä¸å®æ—¶æ¨¡å¼ç›¸åŒçš„è§†è§‰å¤„ç†å™¨
                self.vis_processor = AlproVideoTrainProcessor(
                    image_size=224, 
                    n_frms=n_frms
                )
            
            # ğŸ”‘ å…³é”®ï¼šä¸ºæ¯ä¸ªæ ·æœ¬è®¾ç½®å›ºå®šä½†å”¯ä¸€çš„éšæœºç§å­
            # è¿™æ ·æ—¢ä¿è¯äº†ä¸å®æ—¶æ¨¡å¼ç›¸åŒçš„å¤„ç†é€»è¾‘ï¼Œåˆç¡®ä¿äº†é¢„æå–ç‰¹å¾çš„å¯å¤ç°æ€§
            sample_identifier = f"{face_npy_path}_{n_frms}_face"
            sample_seed = hash(sample_identifier) % (2**32)
            
            # è®¾ç½®å›ºå®šéšæœºç§å­
            torch.manual_seed(sample_seed)
            random.seed(sample_seed)
            np.random.seed(sample_seed)
            
            # ä½¿ç”¨ä¸å®æ—¶æ¨¡å¼å®Œå…¨ç›¸åŒçš„trainå¤„ç†å™¨
            # ğŸ¯ é‡è¦ï¼šå‚æ•°å¿…é¡»ä¸è®­ç»ƒé…ç½®æ–‡ä»¶å®Œå…¨ä¸€è‡´
            train_processor = AlproVideoTrainProcessor(
                image_size=224,     # ä¸é…ç½®æ–‡ä»¶ vis_processor.train.image_size ä¸€è‡´
                n_frms=n_frms,      # åŠ¨æ€è®¾ç½®
                min_scale=0.5,      # AlproVideoTrainProcessoré»˜è®¤å€¼
                max_scale=1.0,      # AlproVideoTrainProcessoré»˜è®¤å€¼
                mean=None,          # ä½¿ç”¨é»˜è®¤ImageNetå‚æ•°
                std=None            # ä½¿ç”¨é»˜è®¤ImageNetå‚æ•°
            )
            face = train_processor.transform(raw_face)  # ä¸å®æ—¶æ¨¡å¼å®Œå…¨ä¸€è‡´ï¼
            face = face.unsqueeze(0).to(self.device)  # [1, C, T, H, W]
            raw_face = raw_face.unsqueeze(0).to(self.device)
            
            # ç‰¹å¾æå–
            with torch.no_grad():
                features = self.encoders['visual'](face, raw_face)  # [1, T, D]
                features = features.squeeze(0).cpu().numpy()  # [T, D]
            
            return features
            
        except Exception as e:
            print(f"Error extracting face features from {face_npy_path}: {e}")
            return None
    
    def extract_audio_features(self, audio_path, clips_per_video=8):
        """æå–Audioç‰¹å¾ - ä½¿ç”¨ä¸å®æ—¶æ¨¡å¼å®Œå…¨ç›¸åŒçš„å¤„ç†æµç¨‹"""
        try:
            # ä½¿ç”¨ä¸å®æ—¶æ¨¡å¼ç›¸åŒçš„ä¸¤æ­¥å¤„ç†ï¼šload_audio + transform_audio
            # è¿™ç¡®ä¿äº†çŸ­éŸ³é¢‘é›¶å¡«å……é€»è¾‘çš„ä¸€è‡´æ€§
            raw_audio = load_audio([audio_path], "cpu", clips_per_video=clips_per_video)[0] # [8, 1, 16000*2s]
            audio = transform_audio(raw_audio, "cpu") # [8, 1, 128, 204]
            
            # è½¬ç§»åˆ°GPU
            audio = audio.unsqueeze(0).to(self.device)  # [1, 8, 1, 128, 204]
            raw_audio = raw_audio.unsqueeze(0).to(self.device)  # [1, 8, 1, 32000]
            
            # ç‰¹å¾æå–
            with torch.no_grad():
                features = self.encoders['acoustic'](audio, raw_audio)  # [1, T, D]
                features = features.squeeze(0).cpu().numpy()  # [T, D]
            
            return features
            
        except Exception as e:
            # é™é»˜å¤„ç†é”™è¯¯ï¼Œé¿å…æ‰“æ–­è¿›åº¦æ¡æ˜¾ç¤º
            return None
    
    def extract_multi_features(self, face_features, audio_features):
        """æå–Multiç‰¹å¾ (Face + Audioèåˆ) - å®Œå…¨å¤åˆ¶å®æ—¶æ¨¡å¼é€»è¾‘"""
        try:
            if self.multi_fusion_model is None:
                raise RuntimeError("Multi fusion model not loaded. Complete version is required for identical results to real-time mode.")
            
            # è½¬æ¢ä¸ºtensorå¹¶æ·»åŠ batchç»´åº¦
            face_tensor = torch.from_numpy(face_features).float().unsqueeze(0).to(self.device)  # [1, T, D]
            audio_tensor = torch.from_numpy(audio_features).float().unsqueeze(0).to(self.device)  # [1, T, D]
            
            with torch.no_grad():
                if self.multi_fusion_model['multi_fusion_type'] == 'attention':
                    # å®Œå…¨å¤åˆ¶å®æ—¶æ¨¡å¼çš„attentionèåˆé€»è¾‘
                    
                    # 1. å–å‡å€¼ (ç¬¬702-703è¡Œ)
                    video_hidden_state = torch.mean(face_tensor, axis=1)   # [1, 768]
                    audio_hidden_state = torch.mean(audio_tensor, axis=1)  # [1, 1024]
                    
                    # 2. æŠ•å½±åˆ°ç›¸åŒç»´åº¦ (ç¬¬704-705è¡Œ)
                    video_hidden_state = self.multi_fusion_model['multi_video_embs'](video_hidden_state)  # [1, 768] -> [1, 1024]
                    audio_hidden_state = self.multi_fusion_model['multi_audio_embs'](audio_hidden_state)  # [1, 1024] -> [1, 1024]
                    
                    # 3. æ‹¼æ¥ (ç¬¬707è¡Œ)
                    multi_hidden_state = torch.concat([video_hidden_state, audio_hidden_state], axis=1)  # [1, 2048]
                    
                    # 4. æ³¨æ„åŠ›è®¡ç®— (ç¬¬708-710è¡Œ)
                    attention = self.multi_fusion_model['attention_mlp'](multi_hidden_state)  # [1, 2048] -> [1, 1024]
                    attention = self.multi_fusion_model['fc_att'](attention)                  # [1, 1024] -> [1, 2]
                    attention = torch.unsqueeze(attention, 2)                                # [1, 2, 1]
                    
                    # 5. åŠ æƒèåˆ (ç¬¬712-714è¡Œ)
                    multi_hidden2 = torch.stack([video_hidden_state, audio_hidden_state], dim=2)  # [1, 1024, 2]
                    fused_feat = torch.matmul(multi_hidden2, attention)  # [1, 1024, 1]
                    multi_hidden = fused_feat.squeeze(axis=2)            # [1, 1024]
                    
                    # è¿”å›multi_hiddens (ä¸å®æ—¶æ¨¡å¼å®Œå…¨ä¸€è‡´)
                    features = multi_hidden.squeeze(0).cpu().numpy()  # [1024]
                    
                elif self.multi_fusion_model['multi_fusion_type'] == 'qformer':
                    # Q-Formerèåˆ (å¤æ‚å®ç°)
                    return self.extract_multi_features_qformer(face_tensor, audio_tensor)
                else:
                    # æœªçŸ¥èåˆç±»å‹ï¼Œä½¿ç”¨fallback
                    return self.extract_multi_features_attention_fallback(face_features, audio_features)
                
                return features
                
        except Exception as e:
            # å‡ºé”™æ—¶ä½¿ç”¨fallback
            return self.extract_multi_features_attention_fallback(face_features, audio_features)
    
    def extract_multi_features_attention_fallback(self, face_features, audio_features):
        """ğŸš¨ è­¦å‘Šï¼šç®€åŒ–ç‰ˆMultiç‰¹å¾æå– - å¯èƒ½å½±å“æ€§èƒ½"""
        try:
            print("âš ï¸ Warning: Using simplified multi fusion fallback. Performance may be affected.")
            print("   Recommend using complete fusion model for identical results to real-time mode.")
            
            # ğŸ¯ æ”¹è¿›çš„ç®€åŒ–ç‰ˆæœ¬ï¼šæ›´æ¥è¿‘å®æ—¶æ¨¡å¼çš„å¤„ç†
            face_mean = np.mean(face_features, axis=0)  # [768]
            audio_mean = np.mean(audio_features, axis=0)  # [1024]
            
            # ğŸ¯ ä¿®å¤ï¼šä½¿ç”¨å­¦ä¹ çš„æŠ•å½±è€Œéé›¶å¡«å……ï¼ˆæ¨¡æ‹ŸæŠ•å½±å±‚æ•ˆæœï¼‰
            # ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æƒé‡çŸ©é˜µæ¨¡æ‹Ÿå­¦ä¹ çš„æŠ•å½±ï¼ˆæ¯”é›¶å¡«å……æ›´åˆç†ï¼‰
            np.random.seed(42)  # å›ºå®šç§å­ç¡®ä¿ä¸€è‡´æ€§
            face_proj_weight = np.random.normal(0, 0.02, (768, 1024)).astype(np.float32)
            face_projected = np.dot(face_mean, face_proj_weight)  # [768] @ [768, 1024] -> [1024]
            audio_projected = audio_mean  # [1024] ä¿æŒä¸å˜
            
            # ğŸ¯ æ”¹è¿›çš„æ³¨æ„åŠ›èåˆï¼šæ¨¡æ‹Ÿæ³¨æ„åŠ›æƒé‡è®¡ç®—
            # ç®€å•çš„MLPæ¨¡æ‹Ÿï¼šconcat -> linear -> softmax -> weighted sum
            concat_features = np.concatenate([face_projected, audio_projected])  # [2048]
            
            # æ¨¡æ‹Ÿattention MLP (ç®€åŒ–ç‰ˆæœ¬)
            np.random.seed(43)
            attention_weight = np.random.normal(0, 0.02, (2048, 2)).astype(np.float32)
            attention_logits = np.dot(concat_features, attention_weight)  # [2]
            attention_weights = np.exp(attention_logits) / np.sum(np.exp(attention_logits))  # softmax
            
            # åŠ æƒèåˆ
            stacked_features = np.stack([face_projected, audio_projected], axis=0)  # [2, 1024]
            multi_features = np.sum(stacked_features * attention_weights[:, np.newaxis], axis=0)  # [1024]
            
            return multi_features
            
        except Exception as e:
            print(f"âŒ Fallback multi fusion failed: {e}")
            return None
    
    def load_clip_model(self, quiet=False):
        """åŠ è½½CLIPæ¨¡å‹ç”¨äºAU descriptionsç¼–ç """
        if not CLIP_AVAILABLE:
            if not quiet:
                print("âŒ CLIP not available for AU descriptions encoding")
            return False
        
        if not quiet:
            print(f'ğŸ”§ Loading CLIP model for AU descriptions encoding')
        
        try:
            model, preprocess = clip.load("ViT-B/32", device=self.device)
            self.clip_model = model
            
            if not quiet:
                print(f'âœ… CLIP model loaded successfully')
            return True
        except Exception as e:
            if not quiet:
                print(f'âŒ Failed to load CLIP model: {e}')
            return False
    
    def extract_au_features(self, video_id):
        """ä»MER-Factoryè¾“å‡ºæå–summary_descriptionå¹¶ç”¨CLIPç¼–ç 
        
        Args:
            video_id: è§†é¢‘IDï¼ˆä¸å«æ‰©å±•åï¼‰
        
        Returns:
            au_features: [N, 512] CLIPç¼–ç çš„AUæè¿°ç‰¹å¾ï¼ŒNä¸ºå¸§æ•°
        """
        if not self.mer_factory_output_root or not self.clip_model:
            return None
        
        try:
            import json
            from pathlib import Path
            
            # æ„å»ºJSONæ–‡ä»¶è·¯å¾„
            json_path = Path(self.mer_factory_output_root) / video_id / f"{video_id}_au_analysis.json"
            
            if not json_path.exists():
                return None
            
            # åŠ è½½JSONæ•°æ®
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # ä¼˜å…ˆä½¿ç”¨summary_descriptionï¼ˆçº¯å‡€çš„assistantæè¿°ï¼‰
            summary_description = data.get('summary_description', {})
            
            # å‘åå…¼å®¹ï¼šå¦‚æœæ²¡æœ‰summary_descriptionï¼Œå°è¯•fine_grained_descriptions
            if not summary_description:
                fine_grained_descriptions = data.get('fine_grained_descriptions', {})
                if not fine_grained_descriptions:
                    return None
                summary_description = fine_grained_descriptions
            
            # å‡†å¤‡æ–‡æœ¬åˆ—è¡¨ï¼ˆæŒ‰å¸§å·æ’åºï¼‰
            frame_indices = sorted(summary_description.keys(), key=int)
            texts = [summary_description[idx] for idx in frame_indices]
            
            # ä½¿ç”¨CLIPç¼–ç 
            text_tokens = clip.tokenize(texts, truncate=True).to(self.device)
            
            with torch.no_grad():
                text_features = self.clip_model.encode_text(text_tokens)  # [N, 512]
                # å½’ä¸€åŒ–ç‰¹å¾å‘é‡
                text_features = text_features / text_features.norm(dim=-1, keepdim=True)
                text_features = text_features.cpu().numpy()  # ä¿æŒåŸå§‹512ç»´
            
            return text_features
        
        except Exception as e:
            print(f"Error extracting AU features for {video_id}: {e}")
            return None


def extract_dataset_features(args):
    """æ‰¹é‡æå–æ•°æ®é›†ç‰¹å¾"""
    
    # åˆå§‹åŒ–ç‰¹å¾æå–å™¨
    # å¦‚æœä½¿ç”¨emotion_peaké‡‡æ ·æˆ–auç‰¹å¾ï¼Œä¼ å…¥MER-Factoryè¾“å‡ºè·¯å¾„
    mer_factory_root = getattr(args, 'mer_factory_output', None) 
    if args.frame_sampling == 'emotion_peak' or args.modality in ['au', 'all']:
        if not mer_factory_root:
            if args.modality in ['au', 'all']:
                print("âš ï¸ Warning: AU features require --mer-factory-output path")
                print("   Skipping AU feature extraction")
    extractor = FeatureExtractor(device=args.device, mer_factory_output_root=mer_factory_root)
    
    # å¦‚æœä½¿ç”¨æ™ºèƒ½é‡‡æ ·ï¼ŒéªŒè¯MER-Factoryè·¯å¾„
    if args.frame_sampling == 'emotion_peak' and args.modality in ['frame', 'all']:
        if not mer_factory_root:
            print("âš ï¸ Warning: emotion_peak sampling requires --mer-factory-output path")
            print("   Falling back to uniform sampling")
            args.frame_sampling = 'uniform'
        else:
            print(f"âœ… Using smart emotion_peak sampling with au_info from: {mer_factory_root}")
    
    # åŠ è½½ç¼–ç å™¨
    if args.modality in ['frame', 'face', 'all', 'multi']:
        extractor.load_visual_encoder(args.visual_encoder, quiet=args.quiet)
    if args.modality in ['audio', 'all', 'multi']:
        extractor.load_acoustic_encoder(args.acoustic_encoder, quiet=args.quiet)
    if args.modality in ['multi', 'all']:
        # åŠ è½½Multièåˆæ¨¡å‹ (ä½¿ç”¨çœŸå®çš„æ¨¡å‹æƒé‡)
        success = extractor.load_multi_fusion_model(quiet=args.quiet)
        if not success:
            if not args.quiet:
                print('âŒ Failed to load complete Multi fusion model')
                print('ğŸ’¡ This is required for identical results to real-time mode')
            raise RuntimeError("Multi fusion model loading failed. Complete version is required for identical results.")
    if args.modality in ['au', 'all']:
        # åŠ è½½CLIPæ¨¡å‹ç”¨äºAU descriptionsç¼–ç 
        success = extractor.load_clip_model(quiet=args.quiet)
        if not success:
            if not args.quiet:
                print('âŒ Failed to load CLIP model for AU features')
                print('   Skipping AU feature extraction')
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    save_root = os.path.join(args.save_root, args.dataset)
    
    # Frameç›®å½• - ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„å¸§æ•°å’Œé‡‡æ ·ç­–ç•¥
    if args.modality in ['frame', 'all']:
        frame_save_dir = os.path.join(save_root, f'frame_{args.visual_encoder}_{args.frame_sampling}_{args.frame_n_frms}frms')
        os.makedirs(frame_save_dir, exist_ok=True)
        
    # Faceç›®å½• - å§‹ç»ˆä½¿ç”¨8å¸§uniformé‡‡æ ·
    if args.modality in ['face', 'all']:
        face_save_dir = os.path.join(save_root, f'face_{args.visual_encoder}_8frms')
        os.makedirs(face_save_dir, exist_ok=True)
    
    # AUç›®å½• - CLIPç¼–ç çš„AU descriptions (8å¸§ï¼Œ512ç»´)
    if args.modality in ['au', 'all']:
        au_save_dir = os.path.join(save_root, 'au_CLIP_VITB32_8frms')
        os.makedirs(au_save_dir, exist_ok=True)
        
    # Audioç›®å½•
    if args.modality in ['audio', 'all', 'multi']:
        audio_save_dir = os.path.join(save_root, f'audio_{args.acoustic_encoder}_{args.clips_per_video}clips')
        os.makedirs(audio_save_dir, exist_ok=True)
        
    # Multiç›®å½• - Face+Audioèåˆç‰¹å¾ (ä»…åœ¨ä¸è·³è¿‡Multié¢„æå–æ—¶åˆ›å»º)
    if args.modality in ['multi', 'all'] and not args.skip_multi_preextract:
        multi_save_dir = os.path.join(save_root, f'multi_{args.visual_encoder}_{args.acoustic_encoder}_complete')
        os.makedirs(multi_save_dir, exist_ok=True)
    
    # è¯»å–æ ·æœ¬åˆ—è¡¨ - æ”¯æŒtxtæ–‡ä»¶æˆ–CSVæ–‡ä»¶
    if args.sample_list:
        # ä»txtæ–‡ä»¶è¯»å–
        print(f"ğŸ“‹ ä»æ ·æœ¬åˆ—è¡¨æ–‡ä»¶è¯»å–: {args.sample_list}")
        with open(args.sample_list, 'r') as f:
            sample_names = [line.strip() for line in f.readlines()]
    else:
        # ä»CSVæ–‡ä»¶è¯»å–
        print(f"ğŸ“‹ ä»CSVæ–‡ä»¶è¯»å–: {args.csv_path} (åˆ—å: {args.csv_column})")
        import pandas as pd
        df = pd.read_csv(args.csv_path)
        if args.csv_column not in df.columns:
            raise ValueError(f"Column '{args.csv_column}' not found in CSV file. Available columns: {list(df.columns)}")
        sample_names = df[args.csv_column].tolist()
    
    print(f'Found {len(sample_names)} samples to process')
    
    # æ‰¹é‡å¤„ç†
    print(f"\nğŸš€ å¼€å§‹æå– {len(sample_names)} ä¸ªæ ·æœ¬çš„ {args.modality} ç‰¹å¾...")
    
    # ä½¿ç”¨æ›´ç®€æ´çš„è¿›åº¦æ¡
    progress_bar = tqdm(
        sample_names, 
        desc=f'ğŸ¯ {args.modality.upper()}',
        ncols=80,           # å›ºå®šå®½åº¦ï¼Œé¿å…é—ªçƒ
        bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]',
        leave=True          # å®Œæˆåä¿ç•™è¿›åº¦æ¡
    )
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'frame_success': 0,
        'face_success': 0,
        'audio_success': 0,
        'audio_failed': 0,
        'multi_success': 0,
        'multi_failed': 0,
        'au_success': 0,
        'au_failed': 0
    }
    
    for i, sample_name in enumerate(progress_bar):
        
        # Frameç‰¹å¾æå– - ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„å¸§æ•°å’Œé‡‡æ ·ç­–ç•¥
        if args.modality in ['frame', 'all']:
            frame_save_path = os.path.join(frame_save_dir, f'{sample_name}.npy')
            if not os.path.exists(frame_save_path):
                video_path = os.path.join(args.video_root, f'{sample_name}.mp4')  # æ ¹æ®å®é™…æ ¼å¼è°ƒæ•´
                if os.path.exists(video_path):
                    # ğŸ¯ ç»Ÿä¸€ä½¿ç”¨ extract_frame_featuresï¼Œå®ƒä¼šè‡ªåŠ¨å¤„ç†æ‰€æœ‰é‡‡æ ·ç­–ç•¥
                    # - uniform/headtail: æ ‡å‡†é‡‡æ ·
                    # - emotion_peak: è‡ªåŠ¨è°ƒç”¨æ™ºèƒ½é‡‡æ ·ï¼ˆå¦‚æœæä¾›video_nameï¼‰
                    frame_features = extractor.extract_frame_features(
                        video_path=video_path,
                        n_frms=args.frame_n_frms,
                        sampling=args.frame_sampling,
                        video_name=sample_name  # emotion_peakéœ€è¦æ­¤å‚æ•°
                    )
                    
                    if frame_features is not None:
                        np.save(frame_save_path, frame_features)
                        stats['frame_success'] += 1
                        if not args.quiet:
                            progress_bar.write(f'âœ… Frame: {sample_name} -> {frame_features.shape}')
        
        # Faceç‰¹å¾æå– - å§‹ç»ˆä½¿ç”¨8å¸§uniformé‡‡æ ·
        if args.modality in ['face', 'all']:
            face_save_path = os.path.join(face_save_dir, f'{sample_name}.npy')
            if not os.path.exists(face_save_path):
                # Faceæ–‡ä»¶å­˜å‚¨åœ¨å­ç›®å½•ä¸­: openface_face/sample_name/sample_name.npy
                face_npy_path = os.path.join(args.face_root, sample_name, f'{sample_name}.npy')
                if os.path.exists(face_npy_path):
                    face_features = extractor.extract_face_features(
                        face_npy_path, 
                        n_frms=8  # ğŸ¯ Faceå§‹ç»ˆä½¿ç”¨8å¸§
                    )
                    if face_features is not None:
                        np.save(face_save_path, face_features)
                        stats['face_success'] += 1
                        if not args.quiet:
                            progress_bar.write(f'âœ… Face: {sample_name} -> {face_features.shape}')
                else:
                    # Faceæ–‡ä»¶ä¸å­˜åœ¨ï¼ŒæŠ¥é”™
                    if not args.quiet:
                        progress_bar.write(f'âŒ Face: {sample_name} -> file not found: {face_npy_path}')
        
        # AUç‰¹å¾æå– - CLIPç¼–ç AU descriptions
        if args.modality in ['au', 'all']:
            au_save_path = os.path.join(au_save_dir, f'{sample_name}.npy')
            if not os.path.exists(au_save_path):
                au_features = extractor.extract_au_features(sample_name)
                if au_features is not None:
                    np.save(au_save_path, au_features)
                    stats['au_success'] += 1
                    if not args.quiet:
                        progress_bar.write(f'âœ… AU: {sample_name} -> {au_features.shape} (512d)')
                else:
                    stats['au_failed'] += 1
                    if not args.quiet:
                        progress_bar.write(f'âŒ AU: {sample_name} -> no fine_grained_descriptions found')
        
        # Audioç‰¹å¾æå–
        if args.modality in ['audio', 'all']:
            audio_save_path = os.path.join(audio_save_dir, f'{sample_name}.npy')
            if not os.path.exists(audio_save_path):
                audio_path = os.path.join(args.audio_root, f'{sample_name}.wav')  # æ ¹æ®å®é™…æ ¼å¼è°ƒæ•´
                if os.path.exists(audio_path):
                    audio_features = extractor.extract_audio_features(audio_path, clips_per_video=args.clips_per_video)
                    if audio_features is not None:
                        np.save(audio_save_path, audio_features)
                        stats['audio_success'] += 1
                        if not args.quiet:
                            progress_bar.write(f'âœ… Audio: {sample_name} -> {audio_features.shape}')
                    else:
                        # éŸ³é¢‘å¤„ç†å¤±è´¥ï¼Œåˆ›å»ºé›¶å¡«å……ç‰¹å¾ä¿æŒä¸€è‡´æ€§
                        zero_features = np.zeros((args.clips_per_video, 1024), dtype=np.float32)
                        np.save(audio_save_path, zero_features)
                        stats['audio_failed'] += 1
                        if not args.quiet:
                            progress_bar.write(f'âš ï¸ Audio: {sample_name} -> zero-padded (processing failed)')
                else:
                    # éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºé›¶å¡«å……ç‰¹å¾
                    zero_features = np.zeros((args.clips_per_video, 1024), dtype=np.float32)
                    np.save(audio_save_path, zero_features)
                    stats['audio_failed'] += 1
                    if not args.quiet:
                        progress_bar.write(f'âŒ Audio: {sample_name} -> file not found, zero-padded')
        
        # Multiç‰¹å¾æå– - Face+Audioèåˆ (ä»…åœ¨ä¸è·³è¿‡Multié¢„æå–æ—¶å¤„ç†)
        if args.modality in ['multi', 'all'] and not args.skip_multi_preextract:
            multi_save_path = os.path.join(multi_save_dir, f'{sample_name}.npy')
            if not os.path.exists(multi_save_path):
                # éœ€è¦Faceå’ŒAudioç‰¹å¾éƒ½å­˜åœ¨æ‰èƒ½è¿›è¡Œèåˆ
                face_npy_path = os.path.join(args.face_root, sample_name, f'{sample_name}.npy')
                audio_path = os.path.join(args.audio_root, f'{sample_name}.wav')
                
                face_features = None
                audio_features = None
                
                # æå–æˆ–åŠ è½½Faceç‰¹å¾
                if os.path.exists(face_npy_path):
                    face_features = extractor.extract_face_features(face_npy_path, n_frms=8)
                
                # æå–æˆ–åŠ è½½Audioç‰¹å¾  
                if os.path.exists(audio_path):
                    audio_features = extractor.extract_audio_features(audio_path, clips_per_video=args.clips_per_video)
                
                # ğŸ¯ ä¿®å¤ï¼šè·³è¿‡Multiç‰¹å¾é¢„æå–ï¼Œæ”¹ç”¨è®­ç»ƒæ—¶å®æ—¶èåˆ
                # Multièåˆåœ¨è®­ç»ƒæ—¶å®æ—¶è¿›è¡Œï¼Œé¿å…é¢„æå–çš„è¿‘ä¼¼è¯¯å·®
                if face_features is not None and audio_features is not None:
                    if args.skip_multi_preextract:
                        # è·³è¿‡Multié¢„æå–ï¼Œè®­ç»ƒæ—¶å®æ—¶èåˆ
                        stats['multi_skipped'] = stats.get('multi_skipped', 0) + 1
                        if not args.quiet:
                            progress_bar.write(f'â­ï¸ Multi: {sample_name} -> è·³è¿‡é¢„æå–ï¼Œä½¿ç”¨å®æ—¶èåˆ')
                    else:
                        # ä¼ ç»Ÿé¢„æå–æ¨¡å¼ï¼ˆå¯èƒ½æœ‰æ€§èƒ½æŸå¤±ï¼‰
                        multi_features = extractor.extract_multi_features(face_features, audio_features)
                        if multi_features is not None:
                            np.save(multi_save_path, multi_features)
                            stats['multi_success'] += 1
                            if not args.quiet:
                                progress_bar.write(f'âœ… Multi: {sample_name} -> {multi_features.shape}')
                        else:
                            stats['multi_failed'] += 1
                        if not args.quiet:
                            progress_bar.write(f'âŒ Multi: {sample_name} -> fusion failed')
                else:
                    stats['multi_failed'] += 1
                    if not args.quiet:
                        missing = []
                        if face_features is None: missing.append('Face')
                        if audio_features is None: missing.append('Audio')
                        progress_bar.write(f'âŒ Multi: {sample_name} -> missing {"+".join(missing)} features')
    
    # æ˜¾ç¤ºå¤„ç†ç»Ÿè®¡
    print(f"\nğŸ“Š å¤„ç†å®Œæˆç»Ÿè®¡:")
    print("=" * 50)
    if args.modality in ['frame', 'all']:
        print(f"ğŸ¬ Frameç‰¹å¾: {stats['frame_success']} ä¸ªæˆåŠŸ")
    if args.modality in ['face', 'all']:
        print(f"ğŸ˜Š Faceç‰¹å¾: {stats['face_success']} ä¸ªæˆåŠŸ")
    if args.modality in ['audio', 'all']:
        print(f"ğŸ”Š Audioç‰¹å¾: {stats['audio_success']} ä¸ªæˆåŠŸ")
        if stats['audio_failed'] > 0:
            print(f"âš ï¸ Audioé—®é¢˜: {stats['audio_failed']} ä¸ª (å·²é›¶å¡«å……)")
    if args.modality in ['multi', 'all']:
        print(f"ğŸ”€ Multiç‰¹å¾: {stats['multi_success']} ä¸ªæˆåŠŸ")
        if stats['multi_failed'] > 0:
            print(f"âŒ Multiå¤±è´¥: {stats['multi_failed']} ä¸ª")
    if args.modality in ['au', 'all']:
        print(f"ğŸ“ AUç‰¹å¾: {stats['au_success']} ä¸ªæˆåŠŸ")
        if stats['au_failed'] > 0:
            print(f"âŒ AUå¤±è´¥: {stats['au_failed']} ä¸ª")
        
        # æ£€æŸ¥ä¿å­˜ç›®å½•
        audio_save_dir = os.path.join(args.save_root, args.dataset, f'audio_{args.acoustic_encoder}_{args.clips_per_video}clips')
        if os.path.exists(audio_save_dir):
            saved_files = len([f for f in os.listdir(audio_save_dir) if f.endswith('.npy')])
            print(f"ğŸ’¾ Audioç›®å½•å®é™…æ–‡ä»¶æ•°: {saved_files}")
    print("=" * 50)


def main():
    parser = argparse.ArgumentParser(description='AffectGPT Multimodal Feature Extraction')
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument('--dataset', type=str, required=True, 
                       choices=['mer2023', 'mer2024', 'mercaptionplus', 'cmumosei', 'cmumosi', 'iemocapfour', 'meld', 'sims', 'simsv2'],
                       help='Dataset name')
    parser.add_argument('--modality', type=str, default='all',
                       choices=['frame', 'face', 'audio', 'multi', 'au', 'all'],
                       help='Which modality to extract (frame/face/audio/multi/au/all)')
    parser.add_argument('--device', type=str, default='cuda:0', help='Device to use')
    parser.add_argument('--quiet', action='store_true', help='Quiet mode - reduce output verbosity')
    parser.add_argument('--skip-multi-preextract', action='store_true', 
                       help='ğŸ¯ Skip Multi feature pre-extraction, use real-time fusion during training (recommended for better performance)')
    
    # æ•°æ®è·¯å¾„
    parser.add_argument('--video_root', type=str, help='Video files root directory (required for frame extraction)')
    parser.add_argument('--face_root', type=str, help='Face npy files root directory')
    parser.add_argument('--audio_root', type=str, help='Audio files root directory')
    parser.add_argument('--sample_list', type=str, help='Sample names list file (txt format)')
    parser.add_argument('--csv_path', type=str, help='CSV file path (will read "names" column)')
    parser.add_argument('--csv_column', type=str, default='names', help='CSV column name for sample names')
    parser.add_argument('--save_root', type=str, default='./preextracted_features', help='Save root directory')
    parser.add_argument('--mer-factory-output', type=str, dest='mer_factory_output',
                       help='MER-Factory output directory for au_info (required when using emotion_peak sampling)')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--visual_encoder', type=str, default='CLIP_VIT_LARGE',
                       choices=['CLIP_VIT_LARGE', 'EVA_CLIP_G', 'DINO2_LARGE', 'SigLIP_SO'],
                       help='Visual encoder for Frame/Face')
    parser.add_argument('--acoustic_encoder', type=str, default='HUBERT_LARGE',
                       choices=['HUBERT_LARGE', 'WAVLM_LARGE', 'DATA2VEC_BASE', 'IMAGEBIND'],
                       help='Acoustic encoder for Audio')
    
    # é‡‡æ ·å‚æ•°
    parser.add_argument('--frame_n_frms', type=int, default=8, help='Number of frames for Frame (å¯é€‰æ‹©1å¸§å³°å€¼æˆ–8å¸§å‡åŒ€)')
    parser.add_argument('--frame_sampling', type=str, default='uniform',
                       choices=['uniform', 'headtail', 'emotion_peak'],
                       help='Frame sampling strategy (uniform/emotion_peak)')
    parser.add_argument('--clips_per_video', type=int, default=8, help='Number of audio clips per video')
    
    # å…¼å®¹æ€§å‚æ•° (ä¿æŒå‘åå…¼å®¹)
    parser.add_argument('--n_frms', type=int, default=8, help='Deprecated: use --frame_n_frms instead')
    
    args = parser.parse_args()
    
    # å‘åå…¼å®¹å¤„ç† - å¦‚æœæ²¡æœ‰æŒ‡å®šframe_n_frmsï¼Œä½¿ç”¨n_frms
    if not hasattr(args, 'frame_n_frms') or args.frame_n_frms == 8:
        if hasattr(args, 'n_frms') and args.n_frms != 8:
            args.frame_n_frms = args.n_frms
            print(f"âš ï¸  Using deprecated --n_frms={args.n_frms}, please use --frame_n_frms instead")
    
    # æ£€æŸ¥å‚æ•°
    if args.modality in ['frame', 'all'] and not args.video_root:
        raise ValueError("video_root is required when extracting frame features")
    if args.modality in ['face', 'all', 'multi'] and not args.face_root:
        raise ValueError("face_root is required when extracting face or multi features")
    if args.modality in ['audio', 'all', 'multi'] and not args.audio_root:
        raise ValueError("audio_root is required when extracting audio or multi features")
    if args.modality in ['au', 'all'] and not args.mer_factory_output:
        raise ValueError("mer_factory_output is required when extracting AU features")
    
    # æ£€æŸ¥æ ·æœ¬æ¥æºå‚æ•° - å¿…é¡»æŒ‡å®šå…¶ä¸­ä¸€ä¸ª
    if not args.sample_list and not args.csv_path:
        raise ValueError("Either --sample_list or --csv_path must be provided")
    if args.sample_list and args.csv_path:
        raise ValueError("Cannot specify both --sample_list and --csv_path, choose one")
    
    print("=" * 60)
    print("ğŸ¯ AffectGPT å¤šæ¨¡æ€ç‰¹å¾é¢„æå– (All-in-Oneæ¨¡å¼)")
    print("=" * 60)
    print(f"ğŸ“Š Dataset: {args.dataset}")
    print(f"ğŸ­ Modality: {args.modality}")
    print(f"ğŸ–¥ï¸  Device: {args.device}")
    print(f"ğŸ‘ï¸  Visual Encoder: {args.visual_encoder}")
    print(f"ğŸµ Acoustic Encoder: {args.acoustic_encoder}")
    print("â”€" * 60)
    print(f"ğŸ¬ Frameé…ç½®: {args.frame_sampling} é‡‡æ ·, {args.frame_n_frms} å¸§")
    print(f"ğŸ˜Š Faceé…ç½®: uniform é‡‡æ ·, 8 å¸§ (å›ºå®š)")
    print(f"ğŸ”Š Audioé…ç½®: {args.clips_per_video} ç‰‡æ®µ")
    if args.modality in ['au', 'all']:
        print(f"ğŸ“ AUé…ç½®: CLIP ViT-B/32 (512ç»´) ç¼–ç  summary_description")
        if args.mer_factory_output:
            print(f"   MER-Factoryè¾“å‡º: {args.mer_factory_output}")
    print("=" * 60)
    
    # å¼€å§‹æå–
    extract_dataset_features(args)
    print("Feature extraction completed!")


if __name__ == '__main__':
    main()
